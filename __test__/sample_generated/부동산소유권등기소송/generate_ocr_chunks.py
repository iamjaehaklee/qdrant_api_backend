#!/usr/bin/env python3
"""
Generate OCR chunk JSON files with 3-page sliding windows for test data.
Each chunk represents a 3-page window with 1-page overlap for context preservation.
"""

import json
import random
import uuid
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Any
import os

# Base directories
SCRIPT_DIR = Path(__file__).parent
SUMMARIES_DIR = SCRIPT_DIR / "ocr_summaries"
CHUNKS_OUTPUT_DIR = SCRIPT_DIR / "ocr_chunks"

# Ensure output directory exists
CHUNKS_OUTPUT_DIR.mkdir(exist_ok=True)

# Korean legal text samples for realistic paragraph generation
LEGAL_TEXT_SAMPLES = [
    "ì›ê³ ëŠ” í”¼ê³ ë¥¼ ìƒëŒ€ë¡œ ë¶€ë™ì‚° ì†Œìœ ê¶Œì´ì „ë“±ê¸° ì²­êµ¬ì˜ ì†Œë¥¼ ì œê¸°í•˜ì˜€ìŠµë‹ˆë‹¤.",
    "ë§¤ë§¤ê³„ì•½ì„œì— ë”°ë¥´ë©´ ë§¤ë§¤ëŒ€ê¸ˆì€ ì´ 5ì–µì›ìœ¼ë¡œ ê³„ì•½ê¸ˆ, ì¤‘ë„ê¸ˆ, ì”ê¸ˆìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.",
    "í”¼ê³ ëŠ” ê³„ì•½ ë‹¹ì‹œ í•´ë‹¹ ë¶€ë™ì‚°ì˜ ì†Œìœ ê¶Œì´ ë³¸ì¸ì—ê²Œ ìˆìŒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.",
    "ì›ê³ ê°€ ì œì¶œí•œ ì¦ê±°ì— ì˜í•˜ë©´ ê³„ì•½ê¸ˆ 5ì²œë§Œì›ì´ 2024ë…„ 3ì›” 15ì¼ ì§€ê¸‰ë˜ì—ˆìŠµë‹ˆë‹¤.",
    "ë¶€ë™ì‚° ë“±ê¸°ë¶€ë“±ë³¸ìƒ ê·¼ì €ë‹¹ê¶Œ ì„¤ì • ì‚¬ì‹¤ì´ í™•ì¸ë©ë‹ˆë‹¤.",
    "ë²•ì›ì€ ë‹¹ì‚¬ìë“¤ì˜ ì£¼ì¥ì„ ê²€í† í•œ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì´ íŒë‹¨í•©ë‹ˆë‹¤.",
    "ê³„ì•½ì„œ ì œ5ì¡°ì— ë”°ë¥´ë©´ ì”ê¸ˆì€ ì†Œìœ ê¶Œì´ì „ë“±ê¸°ì™€ ë™ì‹œì— ì§€ê¸‰í•˜ê¸°ë¡œ ì•½ì •í•˜ì˜€ìŠµë‹ˆë‹¤.",
    "ì›ê³ ëŠ” ê³„ì•½ìƒ ì˜ë¬´ë¥¼ ëª¨ë‘ ì´í–‰í•˜ì˜€ìœ¼ë‚˜ í”¼ê³ ê°€ ë“±ê¸°ë¥¼ í•´ì£¼ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤.",
    "ì¦ì¸ ì‹ ë¬¸ ê²°ê³¼ ê³„ì•½ ë‹¹ì‹œ ìŒë°©ì˜ ì§„ì •í•œ ì˜ì‚¬ëŠ” ë§¤ë§¤ê³„ì•½ ì²´ê²°ì— ìˆì—ˆë˜ ê²ƒìœ¼ë¡œ ì¸ì •ë©ë‹ˆë‹¤.",
    "ê°ì •í‰ê°€ì„œì— ì˜í•˜ë©´ í•´ë‹¹ ë¶€ë™ì‚°ì˜ ì‹œê°€ëŠ” ì•½ 4ì–µ 8ì²œë§Œì›ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.",
]

LEGAL_HEADERS = [
    "1. ì²­êµ¬ì·¨ì§€",
    "2. ì²­êµ¬ì›ì¸",
    "ê°€. ê³„ì•½ ì²´ê²° ê²½ìœ„",
    "ë‚˜. ëŒ€ê¸ˆ ì§€ê¸‰ ì‚¬ì‹¤",
    "ë‹¤. í”¼ê³ ì˜ ì˜ë¬´ ë¶ˆì´í–‰",
    "3. ì¦ê±°ë°©ë²•",
    "4. ë²•ì›ì˜ íŒë‹¨",
    "ê°€. ì¸ì •ì‚¬ì‹¤",
    "ë‚˜. ë²•ë¥ ì  ê²€í† ",
    "5. ê²°ë¡ ",
]


def generate_paragraph(text: str, page: int, idx_in_page: int) -> Dict[str, Any]:
    """Generate a paragraph object with realistic Korean legal text."""
    return {
        "paragraph_id": str(uuid.uuid4()),
        "idx_in_page": idx_in_page,
        "text": text,
        "page": page,
        "bbox": {
            "x": random.uniform(50, 100),
            "y": random.uniform(100, 200) + (idx_in_page * 150),
            "width": random.uniform(1200, 1400),
            "height": random.uniform(80, 150)
        },
        "type": "body" if idx_in_page > 0 else "heading",
        "confidence_score": random.uniform(0.92, 0.99)
    }


def generate_paragraphs_for_page(page: int, num_paragraphs: int = None) -> List[Dict[str, Any]]:
    """Generate realistic paragraphs for a single page."""
    if num_paragraphs is None:
        num_paragraphs = random.randint(3, 8)

    paragraphs = []
    for idx in range(num_paragraphs):
        if idx == 0 and random.random() < 0.3:  # 30% chance of header
            text = random.choice(LEGAL_HEADERS)
        else:
            text = random.choice(LEGAL_TEXT_SAMPLES)

        paragraphs.append(generate_paragraph(text, page, idx))

    return paragraphs


def calculate_windows(total_pages: int) -> List[List[int]]:
    """
    Calculate 3-page sliding windows with 1-page overlap.

    Examples:
    - 5 pages: [1,2,3], [3,4,5]
    - 10 pages: [1,2,3], [3,4,5], [5,6,7], [7,8,9], [9,10]
    - 14 pages: [1,2,3], [3,4,5], [5,6,7], [7,8,9], [9,10,11], [11,12,13], [13,14]
    """
    windows = []
    start = 1

    while start <= total_pages:
        if start + 2 <= total_pages:
            # Full 3-page window
            windows.append([start, start + 1, start + 2])
            start += 2  # Move by 2 for 1-page overlap
        elif start + 1 <= total_pages:
            # 2-page window at the end
            windows.append([start, start + 1])
            break
        else:
            # Single page at the end
            windows.append([start])
            break

    return windows


def generate_chunk(
    summary_data: Dict[str, Any],
    window_pages: List[int],
    chunk_number: int,
    total_pages: int,
    doc_name: str
) -> Dict[str, Any]:
    """Generate a single OCR chunk for a page window."""

    # Generate paragraphs for all pages in the window
    all_paragraphs = []
    for page in window_pages:
        page_paragraphs = generate_paragraphs_for_page(page)
        all_paragraphs.extend(page_paragraphs)

    # Extract paragraph texts
    paragraph_texts = [p["text"] for p in all_paragraphs]

    # Create page dimensions
    page_dimensions = []
    for page in window_pages:
        page_dimensions.append({
            "page": page,
            "width": 1681,
            "height": 2379
        })

    # Build chunk data
    chunk_id = str(uuid.uuid4())

    chunk = {
        "chunk_id": chunk_id,
        "file_id": summary_data["file_id"],
        "project_id": summary_data["project_id"],
        "storage_file_name": f"{uuid.uuid4()}-{int(datetime.now().timestamp() * 1000)}.pdf",
        "original_file_name": f"{doc_name}.pdf",
        "mime_type": "application/pdf",
        "total_pages": total_pages,
        "processing_duration_seconds": 0,
        "language": "ko",
        "pages": window_pages,
        "chunk_number": chunk_number,
        "paragraph_texts": paragraph_texts,
        "chunk_content": {
            "paragraphs": all_paragraphs
        },
        "page_dimensions": page_dimensions,
        "created_at": datetime.now(timezone.utc).isoformat()
    }

    return chunk


def process_summary_file(summary_path: Path, total_pages: int = None) -> List[Dict[str, Any]]:
    """Process a single summary file and generate all its chunks."""

    # Read summary data
    with open(summary_path, 'r', encoding='utf-8') as f:
        summary_data = json.load(f)

    # Assign random page count if not provided
    if total_pages is None:
        total_pages = random.randint(5, 20)

    # Calculate windows
    windows = calculate_windows(total_pages)

    # Generate chunks
    doc_name = summary_path.stem
    chunks = []

    for chunk_number, window_pages in enumerate(windows, start=1):
        chunk = generate_chunk(
            summary_data,
            window_pages,
            chunk_number,
            total_pages,
            doc_name
        )
        chunks.append(chunk)

        # Save chunk to file
        page_range = f"p{window_pages[0]}-{window_pages[-1]}"
        output_filename = f"{doc_name}_{page_range}.json"
        output_path = CHUNKS_OUTPUT_DIR / output_filename

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(chunk, f, ensure_ascii=False, indent=2)

        print(f"  âœ“ Generated: {output_filename}")

    return chunks


def process_batch(batch_name: str, summary_files: List[str], total_pages_override: Dict[str, int] = None):
    """Process a batch of summary files."""
    print(f"\n{'='*60}")
    print(f"Processing {batch_name}")
    print(f"{'='*60}")

    total_override = total_pages_override or {}

    for filename in summary_files:
        summary_path = SUMMARIES_DIR / filename

        if not summary_path.exists():
            print(f"  âš ï¸  File not found: {filename}")
            continue

        print(f"\nğŸ“„ {filename}")

        # Get page count override if specified
        total_pages = total_override.get(filename)

        try:
            chunks = process_summary_file(summary_path, total_pages)
            print(f"  â†’ Generated {len(chunks)} chunks")
        except Exception as e:
            print(f"  âŒ Error: {e}")


def main():
    """Main execution function."""

    print("\n" + "="*60)
    print("OCR Chunk Generator for ë¶€ë™ì‚°ì†Œìœ ê¶Œë“±ê¸°ì†Œì†¡ Case")
    print("="*60)
    print(f"Summaries directory: {SUMMARIES_DIR}")
    print(f"Output directory: {CHUNKS_OUTPUT_DIR}")

    # Batch 1: Core litigation documents (15 docs)
    batch1_files = [
        "ì†Œì¥.json",
        "ë‹µë³€ì„œ.json",
        "ì›ê³ _ì¤€ë¹„ì„œë©´_1ì°¨.json",
        "í”¼ê³ _ì¤€ë¹„ì„œë©´_1ì°¨.json",
        "ì›ê³ _ì¤€ë¹„ì„œë©´_2ì°¨.json",
        "í”¼ê³ _ì¤€ë¹„ì„œë©´_2ì°¨.json",
        "ì›ê³ _ì¤€ë¹„ì„œë©´_3ì°¨_ìµœì¢….json",
        "í”¼ê³ _ì¤€ë¹„ì„œë©´_3ì°¨_ìµœì¢….json",
        "ë¶€ë™ì‚°_ë§¤ë§¤ê³„ì•½ì„œ_ì›ë³¸.json",
        "ë³´ì¶©ê³„ì•½ì„œ_0318.json",
        "ì¤‘ê°œì—…ì_ê±°ë˜í™•ì¸ì„œ.json",
        "ê°‘1í˜¸ì¦_ë§¤ë§¤ê³„ì•½ì„œ.json",
        "ê°‘2í˜¸ì¦_ê³„ì•½ê¸ˆì˜ìˆ˜ì¦.json",
        "ê°‘3í˜¸ì¦_ì¤‘ë„ê¸ˆ1ì°¨ì˜ìˆ˜ì¦.json",
        "ê°‘4í˜¸ì¦_ì¤‘ë„ê¸ˆ2ì°¨ì˜ìˆ˜ì¦.json",
    ]

    # Batch 2: ê°‘ series evidence (15 docs)
    batch2_files = [
        "ê°‘5í˜¸ì¦_ì”ê¸ˆì…ê¸ˆí™•ì¸ì¦.json",
        "ê°‘6í˜¸ì¦_ê³„ì¢Œê±°ë˜ë‚´ì—­_ê³„ì•½ê¸ˆ.json",
        "ê°‘7í˜¸ì¦_ê³„ì¢Œê±°ë˜ë‚´ì—­_ì¤‘ë„ê¸ˆ1ì°¨.json",
        "ê°‘8í˜¸ì¦_ê³„ì¢Œê±°ë˜ë‚´ì—­_ì¤‘ë„ê¸ˆ2ì°¨.json",
        "ê°‘9í˜¸ì¦_ê³„ì¢Œê±°ë˜ë‚´ì—­_ì”ê¸ˆ.json",
        "ê°‘10í˜¸ì¦_ë‚´ìš©ì¦ëª…_1ì°¨_ì´í–‰ì´‰êµ¬.json",
        "ê°‘11í˜¸ì¦_ë‚´ìš©ì¦ëª…_2ì°¨_ìµœê³ .json",
        "ê°‘12í˜¸ì¦_ë“±ê¸°ë¶€ë“±ë³¸.json",
        "ê°‘13í˜¸ì¦_ê±´ì¶•ë¬¼ëŒ€ì¥.json",
        "ê°‘14í˜¸ì¦_ì¹´ì¹´ì˜¤í†¡ëŒ€í™”_ê³„ì•½ë…¼ì˜.json",
        "ê°‘15í˜¸ì¦_ì¹´ì¹´ì˜¤í†¡ëŒ€í™”_ì´í–‰ìš”êµ¬.json",
        "ê°‘16í˜¸ì¦_ë¬¸ìë©”ì‹œì§€.json",
        "ê°‘17í˜¸ì¦_ê³µì¸ì¤‘ê°œì‚¬í™•ì¸ì„œ.json",
        "ê°‘18í˜¸ì¦_ë¶€ë™ì‚°ê°ì •í‰ê°€ì„œ.json",
        "ê°‘19í˜¸ì¦_ì›ê³ ì¸ê°ì¦ëª…ì„œ.json",
    ]

    # Batch 3: More ê°‘ and ì„ series evidence (10 docs)
    batch3_files = [
        "ê°‘20í˜¸ì¦_ì›ê³ ì£¼ë¯¼ë“±ë¡ë“±ë³¸.json",
        "ê°‘21í˜¸ì¦_ëŒ€ì¶œì•½ì •ì„œ.json",
        "ì„1í˜¸ì¦_ë³´ì¶©í•©ì˜ì„œ_ìœ„ì¡°ì˜ì‹¬.json",
        "ì„2í˜¸ì¦_í”¼ê³ ê³„ì¢Œê±°ë˜ë‚´ì—­.json",
        "ì„3í˜¸ì¦_í”¼ê³ ë¬¸ìë©”ì‹œì§€_ì•…ì˜ì ì˜ë„.json",
        "ì„4í˜¸ì¦_ë¶€ë™ì‚°ë‹´ë³´ëŒ€ì¶œì„œë¥˜.json",
        "ì„5í˜¸ì¦_í”¼ê³ ì£¼ë¯¼ë“±ë¡ë“±ë³¸.json",
        "ì„6í˜¸ì¦_í”¼ê³ ì§„ìˆ ì„œ.json",
        "ì„7í˜¸ì¦_ê°ì •í‰ê°€ì„œ_í”¼ê³ ì œì¶œ.json",
        "ì„8í˜¸ì¦_ì œ3ìì§„ìˆ ì„œ.json",
    ]

    # Batch 4: Hearing transcripts and expert reports (10 docs)
    batch4_files = [
        "ì¦ì¸ì‹ ë¬¸ì¡°ì„œ_ì›ê³ ë³¸ì¸.json",
        "ì¦ì¸ì‹ ë¬¸ì¡°ì„œ_í”¼ê³ ë³¸ì¸.json",
        "ì¦ì¸ì‹ ë¬¸ì¡°ì„œ_ê³µì¸ì¤‘ê°œì‚¬.json",
        "ì¦ì¸ì‹ ë¬¸ì¡°ì„œ_ì œ3ìëª©ê²©ì.json",
        "ë³€ë¡ ë…¹ì·¨ë¡_1íšŒ_0903.json",
        "ë³€ë¡ ë…¹ì·¨ë¡_2íšŒ_1008.json",
        "ë³€ë¡ ë…¹ì·¨ë¡_3íšŒ_1105.json",
        "ì¦ê±°ì¡°ì‚¬ê¸°ì¼ë…¹ì·¨ë¡_1112.json",
        "ìµœì¢…ë³€ë¡ ê¸°ì¼ë…¹ì·¨ë¡_1210.json",
        "êµ­ê³¼ìˆ˜í•„ì ê°ì •ì„œ.json",
    ]

    # Process batches
    process_batch("Batch 1: Core Litigation Documents", batch1_files)
    process_batch("Batch 2: ê°‘ Series Evidence", batch2_files)
    process_batch("Batch 3: ì„ Series Evidence", batch3_files)
    process_batch("Batch 4: Hearing Transcripts & Expert Reports", batch4_files)

    # Summary
    print("\n" + "="*60)
    print("Generation Complete!")
    print("="*60)

    total_files = len(list(CHUNKS_OUTPUT_DIR.glob("*.json")))
    print(f"Total chunk files generated: {total_files}")
    print(f"Output location: {CHUNKS_OUTPUT_DIR}")


if __name__ == "__main__":
    main()
